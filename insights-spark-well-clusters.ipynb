{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we're in insights on a local kernel, but through DB connect we can access data on the DBFS\n",
        "path = \"/FileStore/tables/oil_and_ng_wells_hifld_opendata.csv\"\n",
        "df = spark.read.csv(path, header=\"true\", inferSchema=\"true\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looks like there's about 1.5m rows and 35 columns\n",
        "print(df.count(), len(df.columns))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(n=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my \"X\" column was inferred as a string - correcting\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "df = df.withColumn(\"X\", df[\"X\"].cast(DoubleType()))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "cols = [\"X\", \"Y\"]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "  inputCols=cols, \n",
        "  outputCol=\"features\", \n",
        "  handleInvalid=\"skip\"\n",
        ")\n",
        "\n",
        "locations = assembler.transform(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# fit a k-means model using the new \"features\" column\n",
        "km = KMeans(k=50)\n",
        "model = km.fit(locations.select(\"features\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusters = model.transform(locations)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "active = clusters.select([\"prediction\", \"features\", \"API\", \"STATUS\"]).filter(col(\"STATUS\") != \"NON-ACTIVE WELL\")\n",
        "active.show(n=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting active wells - in Insights, you can click the resulting chart and add it directly to a workbook\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(style=\"ticks\", color_codes=True, rc={\"figure.figsize\":(12.7,10.27)})\n",
        "\n",
        "pdf = active.groupBy(\"STATUS\").count().toPandas()\n",
        "pdf['status'] = [\"prod, na\", \"dev\", \"active\", \"unknown\", \"tx\", \"smo\", \"prod\", \"O&G\"] # less verbose labels\n",
        "\n",
        "sns.catplot(x=\"status\", y=\"count\", hue=\"status\", kind=\"bar\", data=pdf)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python3"
    },
    "kernelspec": {
      "argv": [
        "/usr/local/anaconda3/bin/python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "nteract": {
      "version": "0.24.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}